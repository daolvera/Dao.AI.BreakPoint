# -*- coding: utf-8 -*-
"""MovenetPOC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhZCXswvmt_VMCRVLv5Q0Y_ZUq0nK0Qe
"""

# Confidence score to determine whether a keypoint prediction is reliable.
MIN_CROP_KEYPOINT_SCORE = 0.2

def init_crop_region(image_height, image_width):
  """Defines the default crop region.

  The function provides the initial crop region (pads the full image from both
  sides to make it a square image) when the algorithm cannot reliably determine
  the crop region from the previous frame.
  """
  if image_width > image_height:
    box_height = image_width / image_height
    box_width = 1.0
    y_min = (image_height / 2 - image_width / 2) / image_height
    x_min = 0.0
  else:
    box_height = 1.0
    box_width = image_height / image_width
    y_min = 0.0
    x_min = (image_width / 2 - image_height / 2) / image_width

  return {
    'y_min': y_min,
    'x_min': x_min,
    'y_max': y_min + box_height,
    'x_max': x_min + box_width,
    'height': box_height,
    'width': box_width
  }

def torso_visible(keypoints):
  """Checks whether there are enough torso keypoints.

  This function checks whether the model is confident at predicting one of the
  shoulders/hips which is required to determine a good crop region.
  """
  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >
           MIN_CROP_KEYPOINT_SCORE or
          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >
           MIN_CROP_KEYPOINT_SCORE) and
          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >
           MIN_CROP_KEYPOINT_SCORE or
          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >
           MIN_CROP_KEYPOINT_SCORE))

def determine_torso_and_body_range(
    keypoints, target_keypoints, center_y, center_x):
  """Calculates the maximum distance from each keypoints to the center location.

  The function returns the maximum distances from the two sets of keypoints:
  full 17 keypoints and 4 torso keypoints. The returned information will be
  used to determine the crop size. See determineCropRegion for more detail.
  """
  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']
  max_torso_yrange = 0.0
  max_torso_xrange = 0.0
  for joint in torso_joints:
    dist_y = abs(center_y - target_keypoints[joint][0])
    dist_x = abs(center_x - target_keypoints[joint][1])
    if dist_y > max_torso_yrange:
      max_torso_yrange = dist_y
    if dist_x > max_torso_xrange:
      max_torso_xrange = dist_x

  max_body_yrange = 0.0
  max_body_xrange = 0.0
  for joint in KEYPOINT_DICT.keys():
    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:
      continue
    dist_y = abs(center_y - target_keypoints[joint][0]);
    dist_x = abs(center_x - target_keypoints[joint][1]);
    if dist_y > max_body_yrange:
      max_body_yrange = dist_y

    if dist_x > max_body_xrange:
      max_body_xrange = dist_x

  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]

def determine_crop_region(
      keypoints, image_height,
      image_width):
  """Determines the region to crop the image for the model to run inference on.

  The algorithm uses the detected joints from the previous frame to estimate
  the square region that encloses the full body of the target person and
  centers at the midpoint of two hip joints. The crop size is determined by
  the distances between each joints and the center point.
  When the model is not confident with the four torso joint predictions, the
  function returns a default crop which is the full image padded to square.
  """
  target_keypoints = {}
  for joint in KEYPOINT_DICT.keys():
    target_keypoints[joint] = [
      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,
      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width
    ]

  if torso_visible(keypoints):
    center_y = (target_keypoints['left_hip'][0] +
                target_keypoints['right_hip'][0]) / 2;
    center_x = (target_keypoints['left_hip'][1] +
                target_keypoints['right_hip'][1]) / 2;

    (max_torso_yrange, max_torso_xrange,
      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(
          keypoints, target_keypoints, center_y, center_x)

    crop_length_half = np.amax(
        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,
          max_body_yrange * 1.2, max_body_xrange * 1.2])

    tmp = np.array(
        [center_x, image_width - center_x, center_y, image_height - center_y])
    crop_length_half = np.amin(
        [crop_length_half, np.amax(tmp)]);

    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];

    if crop_length_half > max(image_width, image_height) / 2:
      return init_crop_region(image_height, image_width)
    else:
      crop_length = crop_length_half * 2;
      return {
        'y_min': crop_corner[0] / image_height,
        'x_min': crop_corner[1] / image_width,
        'y_max': (crop_corner[0] + crop_length) / image_height,
        'x_max': (crop_corner[1] + crop_length) / image_width,
        'height': (crop_corner[0] + crop_length) / image_height -
            crop_corner[0] / image_height,
        'width': (crop_corner[1] + crop_length) / image_width -
            crop_corner[1] / image_width
      }
  else:
    return init_crop_region(image_height, image_width)

def crop_and_resize(image, crop_region, crop_size):
  """Crops and resize the image to prepare for the model input."""
  boxes=[[crop_region['y_min'], crop_region['x_min'],
          crop_region['y_max'], crop_region['x_max']]]
  output_image = tf.image.crop_and_resize(
      image, box_indices=[0], boxes=boxes, crop_size=crop_size)
  return output_image

def run_inference(movenet, image, crop_region, crop_size):
  """Runs model inference on the cropped region.

  The function runs the model inference on the cropped region and updates the
  model output to the original image coordinate system.
  """
  image_height, image_width, _ = image.shape
  input_image = crop_and_resize(
    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)
  # Run model inference.
  keypoints_with_scores = movenet(input_image)
  # Update the coordinates.
  for idx in range(17):
    keypoints_with_scores[0, 0, idx, 0] = (
        crop_region['y_min'] * image_height +
        crop_region['height'] * image_height *
        keypoints_with_scores[0, 0, idx, 0]) / image_height
    keypoints_with_scores[0, 0, idx, 1] = (
        crop_region['x_min'] * image_width +
        crop_region['width'] * image_width *
        keypoints_with_scores[0, 0, idx, 1]) / image_width
  return keypoints_with_scores

module = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")

def movenet(input_image):
  """Runs detection on an input image.

  Args:
    input_image: A [1, height, width, 3] tensor represents the input image
      pixels. Note that the height/width should already be resized and match the
      expected input resolution of the model before passing into this function.

  Returns:
    A [1, 1, 17, 3] float numpy array representing the predicted keypoint
    coordinates and scores.
  """
  model = module.signatures['serving_default']

  # SavedModel format expects tensor type of int32.
  input_image = tf.cast(input_image, dtype=tf.int32)
  # Run model inference.
  outputs = model(input_image)
  # Output is a [1, 1, 17, 3] tensor.
  keypoints_with_scores = outputs['output_0'].numpy()
  return keypoints_with_scores

import cv2
import numpy as np

video_path = "/content/shortRogerSwing.mp4"
cap = cv2.VideoCapture(video_path)

frames = []
while True:
    ret, frame = cap.read()
    if not ret:
        break
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frames.append(frame)

cap.release()

frames = np.array(frames)

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import cv2

# Import matplotlib libraries
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
import matplotlib.patches as patches

# Some modules to display an animation using imageio.
import imageio
from IPython.display import HTML, display

# Dictionary that maps from joint names to keypoint indices.
KEYPOINT_DICT = {
    'nose': 0,
    'left_eye': 1,
    'right_eye': 2,
    'left_ear': 3,
    'right_ear': 4,
    'left_shoulder': 5,
    'right_shoulder': 6,
    'left_elbow': 7,
    'right_elbow': 8,
    'left_wrist': 9,
    'right_wrist': 10,
    'left_hip': 11,
    'right_hip': 12,
    'left_knee': 13,
    'right_knee': 14,
    'left_ankle': 15,
    'right_ankle': 16
}

# Maps bones to a matplotlib color name.
KEYPOINT_EDGE_INDS_TO_COLOR = {
    (0, 1): 'm',
    (0, 2): 'c',
    (1, 3): 'm',
    (2, 4): 'c',
    (0, 5): 'm',
    (0, 6): 'c',
    (5, 7): 'm',
    (7, 9): 'm',
    (6, 8): 'c',
    (8, 10): 'c',
    (5, 6): 'y',
    (5, 11): 'm',
    (6, 12): 'c',
    (11, 12): 'y',
    (11, 13): 'm',
    (13, 15): 'm',
    (12, 14): 'c',
    (14, 16): 'c'
}

def _keypoints_and_edges_for_display(keypoints_with_scores,
                                     height,
                                     width,
                                     keypoint_threshold=0.11):
  """Returns high confidence keypoints and edges for visualization.

  Args:
    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing
      the keypoint coordinates and scores returned from the MoveNet model.
    height: height of the image in pixels.
    width: width of the image in pixels.
    keypoint_threshold: minimum confidence score for a keypoint to be
      visualized.

  Returns:
    A (keypoints_xy, edges_xy, edge_colors) containing:
      * the coordinates of all keypoints of all detected entities;
      * the coordinates of all skeleton edges of all detected entities;
      * the colors in which the edges should be plotted.
  """
  keypoints_all = []
  keypoint_edges_all = []
  edge_colors = []
  num_instances, _, _, _ = keypoints_with_scores.shape
  for idx in range(num_instances):
    kpts_x = keypoints_with_scores[0, idx, :, 1]
    kpts_y = keypoints_with_scores[0, idx, :, 0]
    kpts_scores = keypoints_with_scores[0, idx, :, 2]
    kpts_absolute_xy = np.stack(
        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)
    kpts_above_thresh_absolute = kpts_absolute_xy[
        kpts_scores > keypoint_threshold, :]
    keypoints_all.append(kpts_above_thresh_absolute)

    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():
      if (kpts_scores[edge_pair[0]] > keypoint_threshold and
          kpts_scores[edge_pair[1]] > keypoint_threshold):
        x_start = kpts_absolute_xy[edge_pair[0], 0]
        y_start = kpts_absolute_xy[edge_pair[0], 1]
        x_end = kpts_absolute_xy[edge_pair[1], 0]
        y_end = kpts_absolute_xy[edge_pair[1], 1]
        line_seg = np.array([[x_start, y_start], [x_end, y_end]])
        keypoint_edges_all.append(line_seg)
        edge_colors.append(color)
  if keypoints_all:
    keypoints_xy = np.concatenate(keypoints_all, axis=0)
  else:
    keypoints_xy = np.zeros((0, 17, 2))

  if keypoint_edges_all:
    edges_xy = np.stack(keypoint_edges_all, axis=0)
  else:
    edges_xy = np.zeros((0, 2, 2))
  return keypoints_xy, edges_xy, edge_colors


def draw_prediction_on_image(
    image, keypoints_with_scores, crop_region=None, close_figure=False,
    output_image_height=None):

    height, width, channel = image.shape
    aspect_ratio = float(width) / height
    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))

    fig.tight_layout(pad=0)
    ax.margins(0)
    ax.set_yticklabels([])
    ax.set_xticklabels([])
    plt.axis('off')

    ax.imshow(image)
    line_segments = LineCollection([], linewidths=(4), linestyle='solid')
    ax.add_collection(line_segments)
    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)

    (keypoint_locs, keypoint_edges, edge_colors) = _keypoints_and_edges_for_display(
        keypoints_with_scores, height, width)

    line_segments.set_segments(keypoint_edges)
    line_segments.set_color(edge_colors)

    if keypoint_locs.shape[0]:
        scat.set_offsets(keypoint_locs)

    # Draw crop region if provided
    if crop_region is not None:
        xmin = max(crop_region['x_min'] * width, 0.0)
        ymin = max(crop_region['y_min'] * height, 0.0)
        rec_width = min(crop_region['x_max'], 0.99) * width - xmin
        rec_height = min(crop_region['y_max'], 0.99) * height - ymin
        rect = patches.Rectangle(
            (xmin, ymin), rec_width, rec_height,
            linewidth=1, edgecolor='b', facecolor='none')
        ax.add_patch(rect)

    fig.canvas.draw()
    buf = fig.canvas.buffer_rgba()

    # Convert to a NumPy array (RGBA)
    w, h = fig.canvas.get_width_height()
    image_rgba = np.frombuffer(buf, dtype=np.uint8).reshape((h, w, 4))

    # Convert RGBA â†’ RGB (drop alpha channel)
    image_rgb = image_rgba[:, :, :3].copy()

    plt.close(fig)

    # Resize output if requested
    if output_image_height is not None:
        output_image_width = int(output_image_height / height * width)
        image_rgb = cv2.resize(
            image_rgb,
            dsize=(output_image_width, output_image_height),
            interpolation=cv2.INTER_CUBIC
        )

    return image_rgb

from IPython.display import Image


def to_gif(images, duration):
  """Converts image sequence (4D numpy array) to gif."""
  imageio.mimsave('./animation.gif', images, duration=duration)
  Image(open("./animation.gif", "rb").read())

def progress(value, max=100):
  return HTML("""
      <progress
          value='{value}'
          max='{max}',
          style='width: 100%'
      >
          {value}
      </progress>
  """.format(value=value, max=max))

num_frames, image_height, image_width, _ = frames.shape
print(num_frames)

# Load the input image.
crop_region = init_crop_region(image_height, image_width)
input_size = 256

output_images = []
for i in range(60):
    # Run MoveNet inference
    keypoints_with_scores = run_inference(
        movenet,
        frames[i],
        crop_region,
        crop_size=[input_size, input_size]
    )

    # Draw skeleton
    out = draw_prediction_on_image(
        frames[i].astype(np.int32),
        keypoints_with_scores,
        crop_region=None,
        close_figure=True,
        output_image_height=300
    )
    output_images.append(out)

    # Update crop region for next frame (tracking)
    crop_region = determine_crop_region(
        keypoints_with_scores,
        image_height,
        image_width
    )

output = np.stack(output_images, axis=0)

to_gif(output, duration=100)

def keypoints_to_pixels(keypoints_with_scores, height, width):
  """Convert MoveNet output to pixel coordinates.

  Returns an array of shape (17, 2) with x,y pixel coordinates and a (17,) confs.
  """
  kpts = keypoints_with_scores[0, 0, :, :]
  ys = kpts[:, 0] * height
  xs = kpts[:, 1] * width
  confs = kpts[:, 2]
  xy = np.stack([xs, ys], axis=1)
  return xy, confs


def angle_between(a, b, c):
  """Return angle (degrees) at point b formed by points a-b-c.

  If vectors are degenerate, returns np.nan.
  """
  v1 = a - b
  v2 = c - b
  n1 = np.linalg.norm(v1)
  n2 = np.linalg.norm(v2)
  if n1 == 0 or n2 == 0:
    return np.nan
  cosang = np.dot(v1, v2) / (n1 * n2)
  cosang = np.clip(cosang, -1.0, 1.0)
  return np.degrees(np.arccos(cosang))


def build_frame_features(prev2_xy, prev_xy, curr_xy, dt, confs=None, conf_thresh=0.2):
  """Compute velocities, accelerations and angles for a single frame.

  Inputs:
    prev2_xy, prev_xy, curr_xy: arrays shape (17,2) or None for unavailable.
    dt: time delta between frames in seconds.

  Returns:
    feature_vector: 1D numpy array containing selected features.
  """
  # Important joints to compute speed/accel for (wrists, elbows, shoulders, hips, knees, ankles)
  joints = [5,6,7,8,9,10,11,12,13,14,15,16]

  # compute vel and acc arrays
  vel = np.zeros_like(curr_xy)
  acc = np.zeros_like(curr_xy)
  if prev_xy is not None:
    vel = (curr_xy - prev_xy) / dt
  if prev2_xy is not None:
    acc = (curr_xy - 2 * prev_xy + prev2_xy) / (dt ** 2)

  feats = []

  # speeds and acc magnitudes for selected joints
  for j in joints:
    v = vel[j]
    a = acc[j]
    speed = np.linalg.norm(v)
    acc_mag = np.linalg.norm(a)
    # mask by confidence if provided
    if confs is not None and confs[j] < conf_thresh:
      speed = np.nan
      acc_mag = np.nan
    feats.append(speed)
    feats.append(acc_mag)

  # angles: elbows, shoulders, hips, knees (both sides)
  # left elbow: shoulder-elbow-wrist -> 5,7,9 (shoulder, elbow, wrist)
  # right elbow: 6,8,10
  # left shoulder: 7,5,11 (elbow, shoulder, hip)
  # right shoulder: 8,6,12
  # left hip: 5,11,13 (shoulder, hip, knee)
  # right hip: 6,12,14
  # left knee: 11,13,15 (hip, knee, ankle)
  # right knee: 12,14,16
  try:
    L_shoulder, R_shoulder = 5, 6
    L_elbow, R_elbow = 7, 8
    L_wrist, R_wrist = 9, 10
    L_hip, R_hip = 11, 12
    L_knee, R_knee = 13, 14
    L_ankle, R_ankle = 15, 16

    angle_left_elbow = angle_between(curr_xy[L_shoulder], curr_xy[L_elbow], curr_xy[L_wrist])
    angle_right_elbow = angle_between(curr_xy[R_shoulder], curr_xy[R_elbow], curr_xy[R_wrist])
    angle_left_shoulder = angle_between(curr_xy[L_elbow], curr_xy[L_shoulder], curr_xy[L_hip])
    angle_right_shoulder = angle_between(curr_xy[R_elbow], curr_xy[R_shoulder], curr_xy[R_hip])
    angle_left_hip = angle_between(curr_xy[L_shoulder], curr_xy[L_hip], curr_xy[L_knee])
    angle_right_hip = angle_between(curr_xy[R_shoulder], curr_xy[R_hip], curr_xy[R_knee])
    angle_left_knee = angle_between(curr_xy[L_hip], curr_xy[L_knee], curr_xy[L_ankle])
    angle_right_knee = angle_between(curr_xy[R_hip], curr_xy[R_knee], curr_xy[R_ankle])
    # mask angles if any involved joint confidence is low
    if confs is not None:
      if confs[L_shoulder] < conf_thresh or confs[L_elbow] < conf_thresh or confs[L_wrist] < conf_thresh:
        angle_left_elbow = np.nan
      if confs[R_shoulder] < conf_thresh or confs[R_elbow] < conf_thresh or confs[R_wrist] < conf_thresh:
        angle_right_elbow = np.nan
      if confs[L_elbow] < conf_thresh or confs[L_shoulder] < conf_thresh or confs[L_hip] < conf_thresh:
        angle_left_shoulder = np.nan
      if confs[R_elbow] < conf_thresh or confs[R_shoulder] < conf_thresh or confs[R_hip] < conf_thresh:
        angle_right_shoulder = np.nan
      if confs[L_shoulder] < conf_thresh or confs[L_hip] < conf_thresh or confs[L_knee] < conf_thresh:
        angle_left_hip = np.nan
      if confs[R_shoulder] < conf_thresh or confs[R_hip] < conf_thresh or confs[R_knee] < conf_thresh:
        angle_right_hip = np.nan
      if confs[L_hip] < conf_thresh or confs[L_knee] < conf_thresh or confs[L_ankle] < conf_thresh:
        angle_left_knee = np.nan
      if confs[R_hip] < conf_thresh or confs[R_knee] < conf_thresh or confs[R_ankle] < conf_thresh:
        angle_right_knee = np.nan
  except Exception:
    angle_left_elbow = angle_right_elbow = angle_left_shoulder = angle_right_shoulder = np.nan
    angle_left_hip = angle_right_hip = angle_left_knee = angle_right_knee = np.nan

  angle_feats = [
    angle_left_elbow, angle_right_elbow,
    angle_left_shoulder, angle_right_shoulder,
    angle_left_hip, angle_right_hip,
    angle_left_knee, angle_right_knee
  ]
  feats.extend(angle_feats)

  # Optionally include flattened joint positions (masked by confidence)
  pos_flat = curr_xy.flatten()
  if confs is not None:
    # expand confidences to match x,y ordering
    confs_xy = np.repeat(confs, 2)
    pos_flat = np.where(confs_xy < conf_thresh, np.nan, pos_flat)
  feats.extend(pos_flat.tolist())

  return np.array(feats, dtype=np.float32)


def normalize_features(features_arr, method='zscore'):
  """Normalize feature array along time axis (rows are frames).

  - fills NaNs with column mean before normalization
  - returns normalized array and scaler params (mean,std)
  """
  # features_arr: shape (T, F)
  feats = features_arr.astype(np.float32).copy()
  # compute column-wise mean/std ignoring NaN
  col_mean = np.nanmean(feats, axis=0)
  col_std = np.nanstd(feats, axis=0)
  # replace NaN with mean
  inds = np.where(np.isnan(feats))
  if inds[0].size > 0:
    feats[inds] = np.take(col_mean, inds[1])
  # avoid zero std
  col_std_safe = np.where(col_std == 0, 1.0, col_std)
  if method == 'zscore':
    feats = (feats - col_mean) / col_std_safe
  elif method == 'maxabs':
    maxabs = np.max(np.abs(feats), axis=0)
    maxabs_safe = np.where(maxabs == 0, 1.0, maxabs)
    feats = feats / maxabs_safe
  else:
    raise ValueError('Unknown normalization method: ' + str(method))
  return feats, col_mean, col_std_safe

def progress(value, max=100):
  return HTML("""
      <progress
          value='{value}'
          max='{max}',
          style='width: 100%'
      >
          {value}
      </progress>
  """.format(value=value, max=max))

num_frames, image_height, image_width, _ = frames.shape
print(num_frames)

# Load the input image.
crop_region = init_crop_region(image_height, image_width)
input_size = 256

output_images = []
features_list = []
prev_xy = None
prev2_xy = None

# iterate up to available frames (or limit to first 60 if desired)
max_frames = min(60, frames.shape[0])
for i in range(max_frames):
  # Run MoveNet inference
  keypoints_with_scores = run_inference(
    movenet,
    frames[i],
    crop_region,
    crop_size=[input_size, input_size]
  )

  # Extract pixel coordinates
  curr_xy, confs = keypoints_to_pixels(keypoints_with_scores, image_height, image_width)

  # Build features (velocities, accelerations, angles, and positions)
  feats = build_frame_features(prev2_xy, prev_xy, curr_xy, dt, confs=confs, conf_thresh=0.2)
  features_list.append(feats)

  # Draw skeleton
  out = draw_prediction_on_image(
    frames[i].astype(np.int32),
    keypoints_with_scores,
    crop_region=None,
    close_figure=True,
    output_image_height=300
  )
  output_images.append(out)

  # Update crop region for next frame (tracking)
  crop_region = determine_crop_region(
    keypoints_with_scores,
    image_height,
    image_width
  )

  # shift histories
  prev2_xy = None if prev_xy is None else prev_xy.copy()
  prev_xy = curr_xy.copy()

output = np.stack(output_images, axis=0)
to_gif(output, duration=100)

# Save feature array for downstream modeling (shape: [frames, features])
if len(features_list) > 0:
  features_arr = np.stack(features_list, axis=0)
  np.save('features.npy', features_arr)
  print('Saved features.npy with shape', features_arr.shape)
  # Normalize and save normalized features + scaler
  feats_norm, mean, std = normalize_features(features_arr, method='zscore')
  np.save('features_norm.npy', feats_norm)
  np.savez('features_scaler.npz', mean=mean, std=std)
  print('Saved features_norm.npy and features_scaler.npz')
else:
  print('No features were generated.')